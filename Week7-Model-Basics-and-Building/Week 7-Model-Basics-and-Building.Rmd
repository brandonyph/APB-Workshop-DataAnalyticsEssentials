---
title: "Week 7-Model Basics and Building"
author: "Brandon"
date: "7/4/2021"
output: html_document
---

Based on http://www.statslab.cam.ac.uk/~pat/redwsheets.pdf

```{r setup, include=FALSE}
#


```

# Regression/Classification model 

1. Linear Regression (GLM)
2. Logistic Regression
3. K-Nearest Neighbors (KNN)
4. Decision Tree
5. Random Forest
6. Support Vector Machine

linear Regression Syntax
lm(y~x+x2+x3+x4, data, subset, weights, na.action, method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE,offset, ...)

```{r Classification model}
#Linear Regression

#https://ucdavis-bioinformatics-training.github.io/2019-March-Bioinformatics-Prerequisites/thursday/linear_models.html

data(iris)

plot(iris$Sepal.Length,iris$Petal.Length)

lmModel <- lm(Petal.Length~Sepal.Length,data = iris)

summary(lmModel)

abline(lmModel)
#######################

new <- data.frame(Sepal.Length = seq(-3, 3, 0.5))
plot(unlist(new), predict(lmModel,new))

```


```{r Classification model}
#Generalised Linear Model
#https://sscc.wisc.edu/sscc/pubs/glm-r/

library(faraway)      # for the hsb dataset
library(MASS)        

data(hsb)
str(hsb)

g1 <- glm(socst ~ gender + race + ses + schtyp + read + write + science ,
          family = poisson(),
          data = hsb)

summary(g1)

```

```{r Classification model}
#Logistic Regression
#https://www.datacamp.com/community/tutorials/logistic-regression-R


#load dataset
data <- ISLR::Default

#view summary of dataset
summary(data)

#make this example reproducible
set.seed(1)

#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- data[sample, ]
test <- data[!sample, ]  

#fit logistic regression model
model <- glm(default~student+balance+income, family="binomial", data=train)

#disable scientific notation for model summary
options(scipen=999)

#view model summary
summary(model)

#define two individuals
new <- data.frame(balance = 1200, income = 10, student = c("Yes", "No"))

#predict probability of defaulting
predict(model, new, type="response")

#calculate probability of default for each individual in test dataset
predicted <- predict(model, test, type="response")

library(InformationValue)
confusionMatrix(test$default, predicted)


```

```{r Classification model}
#K-Nearest Neighbors (KNN)

#https://fderyckel.github.io/machinelearningwithr/knnchapter.html
df <- data(iris) ##load data
head(iris) ## see the studcture

##Generate a random number that is 90% of the total number of rows in dataset.
ran <- sample(1:nrow(iris), 0.9 * nrow(iris))

##the normalization function is created
nor <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

##Run nomalization on first 4 coulumns of dataset because they are the predictors
iris_norm <- as.data.frame(lapply(iris[, c(1, 2, 3, 4)], nor))

summary(iris_norm)
plot(iris_norm)

iris_train <- iris_norm[ran, ]
##extract testing set
iris_test <- iris_norm[-ran, ]

##extract 5th column of train dataset because it will be used as 'cl' argument in knn function.
iris_target_category <- iris[ran, 5]
##extract 5th column if test dataset to measure the accuracy
iris_test_category <- iris[-ran, 5]
##load the package class

library(class)
##run knn function
model_knn <- knn(iris_train, iris_test, cl = iris_target_category, k = 13)

##create confusion matrix
tab <- table(model_knn, iris_test_category)

tab

```

```{r Classification model}
#Decision Tree
#https://www.datacamp.com/community/tutorials/decision-trees-R

# Load the party package. It will automatically load other
# dependent packages.
library(party)

# Create the input data frame.
input.dat <- readingSkills[c(1:105),]

# Create the tree.
output.tree <- ctree(
nativeSpeaker ~ age + shoeSize + score, 
data = input.dat)

# Plot the tree.
plot(output.tree)


```

```{r Classification model}
#Random Forest
library(randomForest)
library(caret)

head(iris)

rf <- randomForest(
  Species ~ .,
  data=iris)

pred = predict(rf,iris[,-5])

confusionMatrix(pred,iris[,5])

#####################################################################################################################################

## 75% of the sample size
smp_size <- floor(0.75 * nrow(iris))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(iris)), size = smp_size)

train <- iris[train_ind, ]
test <- iris[-train_ind, ]

rf2 <- randomForest(
  Species ~ . ,
  data=train)

pred2 = predict(rf2,newdata = test[,-5])

confusionMatrix(pred2,test[,5])

#####################################################################################################################################

plot(rf)
plot(rf2)

```

```{r Classification model}
#Support Vector Machine

##easiest code for sVM

library(e1071)
library(caret)

head(iris)

############################################################################################################
#concepts of SVM, Support Vector Matrix

m2 <- svm(Species~Petal.Width+Petal.Length, data = iris,kernel = 'linear')
plot(m2, iris, Petal.Width ~ Petal.Length,
     slice = list(Sepal.Width = 3, Sepal.Length = 4))

m3 <- svm(Species~Sepal.Width+Sepal.Length, data = iris,kernel = 'linear')
plot(m3, iris, Sepal.Width ~ Sepal.Length,
     slice = list(Sepal.Width = 3, Sepal.Length = 4))

m4 <- svm(Species~Sepal.Width+Sepal.Length, data = iris,kernel = 'radial')
plot(m4, iris, Sepal.Width ~ Sepal.Length,
     slice = list(Sepal.Width = 3, Sepal.Length = 4))

m5 <- svm(Species~Sepal.Width+Sepal.Length, data = iris,kernel = 'polynomial')
plot(m5, iris, Sepal.Width ~ Sepal.Length,
     slice = list(Sepal.Width = 3, Sepal.Length = 4))

############################################################################################################
############################################################################################################

x <- iris[, 1:4]
y <- iris[, 5]

head(x)
head(y)

##Create SVM MODEL
svm_model <- svm(x, y, kernel = "polynomial", scale = TRUE)

summary(svm_model)

pred <- predict(svm_model, x)

confusionMatrix(pred, y)

Comparison <- cbind(iris, as.character(pred))


############################################################################################################
############################################################################################################
############################################################################################################


#gamma decides how many points to consider when drawing the decision boundaries

#Gamma and cost 
  #Higher value resulted in over fitting
  #Low value resulted in overly rigid line

#My guess is that gamma controls how sensitive the boundary 
#would be to outliers, while C has to do directly with the geometry of the boundary. 
#For example, you could have an overfitted boundary that is affected only by the border 
#(high gamma, high C) or a completely straight line affected equally 
#by even the farthest points (low gamma, low C)

tuned.svm <- tune.svm(
  x,y,
  kernel = 'polynomial',
  gamma = seq(1 / 2 ^ nrow(iris), 1, .01),
  cost = 2 ^ seq(-6, 4, 2)
)
tuned.svm

tuned.svm.model <-
  svm(
    x,
    y,
    kernel = 'polynomial',
    gamma = tuned.svm$best.parameters$gamma,
    cost = tuned.svm$best.parameters$cost
  )

pred_2 <- predict(tuned.svm.model, x)

confusionMatrix(pred_2, y)

############################################################################################################
############################################################################################################
#actual way to do

svm_model_linear <- svm(x, y)
pred_3 <- predict(svm_model_linear, x,kernel = 'linear')
confusionMatrix(pred_3, y)

```


# Clustering model
1. Hierarchical 
2. Principle Component Analysis
3. T-SNE
4. UMAP
5. K Means


```{r Clustering model}
#Hierarchical Clustering

library(pheatmap)
data(iris)

iris.scaled <- scale(iris[,1:4])

iris.heatmap <- pheatmap(iris.scaled,
                         kmeans_k = NA, 
                         breaks = NA, 
                         border_color = "grey60",
                         cellwidth = NA, 
                         cellheight = NA, 
                         scale = "none", 
                         cluster_rows = TRUE,
                         cluster_cols = TRUE, 
                         clustering_distance_rows = "euclidean",
                         clustering_distance_cols = "euclidean", 
                         clustering_method = "complete",
                         cutree_rows = 3)

```

```{r Clustering model}
#Principle Component Analysis
library(factoextra)
library(FactoMineR)

data(iris)

iris.pca <- PCA(iris[,1:4])

fviz_pca_var(iris.pca)
fviz_pca_ind(iris.pca,repel = TRUE,col.ind = iris$Species)


```

```{r Clustering model}
# T-SNE 
library(Rtsne)
library(ggplot2)

data(iris)

iris.unique <- iris[!duplicated(iris),]
label <- iris.unique[,5]

iris.unique <- as.matrix(iris.unique[,1:4])

iris.rtsne <- Rtsne(iris.unique, dims = 2, initial_dims = 50, perplexity = 30)

plotdata <- as.data.frame(iris.rtsne$Y)

ggplot(plotdata, aes(x=V1,y=V2)) + geom_point(aes(color=label))

```

```{r Clustering model}
#UMAP 

library(umap)
library(ggplot2)

data(iris)

iris.umap <- umap(iris[,1:4])

plotdata <- data.frame(iris.umap$layout)
  
ggplot(plotdata, aes(x=X1,y=X2)) + geom_point(aes(color=iris$Species))
                 

```

```{r Clustering model}
#K Means
#https://uc-r.github.io/kmeans_clustering

library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization

df <- USArrests
df <- na.omit(df)
df <- scale(df)

distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

k2 <- kmeans(df, centers = 2, nstart = 25)
fviz_cluster(k2, data = df)

df %>%
  as_tibble() %>%
  mutate(cluster = k2$cluster,
         state = row.names(USArrests)) %>%
  ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) +
  geom_text()


k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)

set.seed(123)

fviz_nbclust(df, kmeans, method = "wss")

# Compute k-means clustering with k = 4
set.seed(123)
final <- kmeans(df, 4, nstart = 25)
print(final)
fviz_cluster(final, data = df)

```

# Neural Network
1. MLP (Multilayer Perceptron)
2. Convolutional 
3. Recurrent
4. Auto-encoder (Classification and Dimensional Reduction)
5. Etc (RBM, Transformer, and many more)


```{r Neural Network}
#MLP (Multilayer Perceptron)

library(neuralnet)
library(caret)

head(iris)

NN = neuralnet(Species ~ ., iris, hidden = c(10,10,5) )

# plot neural network
plot(NN)

pred = neuralnet::compute(NN, iris[,c(1,2,3,4)])

pred.2 <- data.frame()
for(i in 1:150){
  pred.2 <- rbind(pred.2 ,which.max(pred$net.result[i,]))
}

pred.2$X1L <- gsub(1,"setosa",pred.2$X1L)
pred.2$X1L <- gsub(2,"versicolor",pred.2$X1L)
pred.2$X1L <- gsub(3,"virginica",pred.2$X1L)

prediction <- as.factor(pred.2$X1L)
reference  <- iris[,5]

confusionMatrix(prediction, reference)


```


```{r Neural Network}
#Convolutional 
library(tensorflow)
library(keras)
library(stringr)
library(readr)
library(purrr)
library(caret)

library(e1071)

cifar <- dataset_cifar10()

train_data <- scale(cifar$train$x)
dim(train_data) <- c(50000,32,32,3)

test_data <- scale(cifar$test$x)
dim(test_data) <- c(10000,32,32,3)

train_label <- as.numeric(cifar$train$y)
dim(train_label) <- c(50000)

test_label <- as.numeric(cifar$test$y)
dim(test_label) <- c(10000)

class_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',
                 'dog', 'frog', 'horse', 'ship', 'truck')

index <- 1:30

par(mfcol = c(5,6), mar = rep(1, 4), oma = rep(0.2, 4))

cifar$train$x[index,,,] %>% 
  purrr::array_tree(1) %>%
  purrr::set_names(class_names[cifar$train$y[index] + 1]) %>% 
  purrr::map(as.raster, max = 255) %>%
  purrr::iwalk(~{plot(.x); title(.y)})

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", input_shape = c(32,32,3)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) 

summary(model)

model %>% 
  layer_flatten() %>% 
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")

summary(model)

model %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

history <- model %>% 
  fit(
    x = train_data, y = train_label,
    epochs = 20,
    validation_split=0.2,
    use_multiprocessing=TRUE
  )

plot(history)

############################################################################
Prediction_train_data <- predict_classes(model, train_data)
confusionMatrix(table(Prediction_train_data,train_label))

```



```{r Neural Network}
#Recurrent
#https://github.com/brandonyph/Introduction-to-RNN-in-R


```


```{r Neural Network}
#Auto-encoder
#https://www.r-bloggers.com/2018/07/pca-vs-autoencoders-for-dimensionality-reduction/

suppressPackageStartupMessages(library(DAAG))
head(ais)

# standardise
minmax <- function(x) (x - min(x))/(max(x) - min(x))
x_train <- apply(ais[,1:11], 2, minmax)

# autoencoder in keras
suppressPackageStartupMessages(library(keras))
# set training data
x_train <- as.matrix(x_train)

# autoencoder in keras
library(keras)
# set training data
###
model3 <- keras_model_sequential()
model3 %>%
  layer_dense(units = 6, activation = "tanh", input_shape = ncol(x_train)) %>%
  layer_dense(units = 3, activation = "tanh", name = "bottleneck") %>%
  layer_dense(units = 6, activation = "tanh") %>%
  layer_dense(units = ncol(x_train))
# summary of model
summary(model3)

# compile model
model3 %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam"
)
# fit model
model3 %>% fit(
  x = x_train, 
  y = x_train, 
  verbose = 0,
  epochs = 1000,
  batch_size = 2
)
# evaluate the performance of the model
mse.ae2 <- evaluate(model3, x_train, x_train)
mse.ae2

# exgtract the bottleneck layer
intermediate_layer_model <- keras_model(inputs = model3$input, outputs = get_layer(model3, "bottleneck")$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
# plot the reduced dat set
aedf3 <- data.frame(node1 = intermediate_output[,1], node2 = intermediate_output[,2], node3 = intermediate_output[,3])

ae_plotly <- plot_ly(aedf3, x = ~node1, y = ~node2, z = ~node3, color = ~ais$sex) %>% add_markers()
ae_plotly 

```



```{r Neural Network}
#Stuff
#https://www.datacamp.com/community/tutorials/machine-learning-in-r



```





















