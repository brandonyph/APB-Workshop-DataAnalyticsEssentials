---
title: "Week 7-Model Basics and Building"
author: "Brandon"
date: "7/4/2021"
output: html_document
---

Based on http://www.statslab.cam.ac.uk/~pat/redwsheets.pdf

```{r setup, include=FALSE}
#

```

# Regression/Classification model 

1. Linear Regression (GLM)
2. Logistic Regression
3. K-Nearest Neighbors (KNN)
4. Decision Tree
5. Random Forest
6. Support Vector Machine

linear Regression Syntax
lm(y~x+x2+x3+x4, data, subset, weights, na.action, method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE,offset, ...)

```{r Classification model}
#Linear Regression

#https://ucdavis-bioinformatics-training.github.io/2019-March-Bioinformatics-Prerequisites/thursday/linear_models.html

data(iris)

plot(iris$Sepal.Length,iris$Petal.Length)

lmModel <- lm(Petal.Length~Sepal.Length,data = iris)

summary(lmModel)

abline(lmModel)
#######################

new <- data.frame(Sepal.Length = seq(-3, 3, 0.5))
plot(unlist(new), predict(lmModel,new))

```


```{r Classification model}
#Generalised Linear Model
#https://sscc.wisc.edu/sscc/pubs/glm-r/

library(faraway)      # for the hsb dataset
library(MASS)        

data(hsb)
str(hsb)

g1 <- glm(socst ~ gender + race + ses + schtyp + read + write + science ,
          family = poisson(),
          data = hsb)

summary(g1)

```

```{r Classification model}
#Logistic Regression
#https://www.datacamp.com/community/tutorials/logistic-regression-R


#load dataset
data <- ISLR::Default

#view summary of dataset
summary(data)

#make this example reproducible
set.seed(1)

#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- data[sample, ]
test <- data[!sample, ]  

#fit logistic regression model
model <- glm(default~student+balance+income, family="binomial", data=train)

#disable scientific notation for model summary
options(scipen=999)

#view model summary
summary(model)

#define two individuals
new <- data.frame(balance = 1200, income = 10, student = c("Yes", "No"))

#predict probability of defaulting
predict(model, new, type="response")

#calculate probability of default for each individual in test dataset
predicted <- predict(model, test, type="response")

library(InformationValue)
confusionMatrix(test$default, predicted)


```

```{r Classification model}
#K-Nearest Neighbors (KNN)

#https://fderyckel.github.io/machinelearningwithr/knnchapter.html
df <- data(iris) ##load data
head(iris) ## see the studcture

##Generate a random number that is 90% of the total number of rows in dataset.
ran <- sample(1:nrow(iris), 0.9 * nrow(iris))

##the normalization function is created
nor <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

##Run nomalization on first 4 coulumns of dataset because they are the predictors
iris_norm <- as.data.frame(lapply(iris[, c(1, 2, 3, 4)], nor))

summary(iris_norm)
plot(iris_norm)

iris_train <- iris_norm[ran, ]
##extract testing set
iris_test <- iris_norm[-ran, ]

##extract 5th column of train dataset because it will be used as 'cl' argument in knn function.
iris_target_category <- iris[ran, 5]
##extract 5th column if test dataset to measure the accuracy
iris_test_category <- iris[-ran, 5]
##load the package class

library(class)
##run knn function
model_knn <- knn(iris_train, iris_test, cl = iris_target_category, k = 13)

##create confusion matrix
tab <- table(model_knn, iris_test_category)

tab

```

```{r Classification model}
#Decision Tree
#https://www.datacamp.com/community/tutorials/decision-trees-R

# Load the party package. It will automatically load other
# dependent packages.
library(party)

# Create the input data frame.
input.dat <- readingSkills[c(1:105),]

# Create the tree.
output.tree <- ctree(
nativeSpeaker ~ age + shoeSize + score, 
data = input.dat)

# Plot the tree.
plot(output.tree)


```

```{r Classification model}
#Random Forest
library(randomForest)
library(caret)

head(iris)

rf <- randomForest(
  Species ~ .,
  data=iris)

pred = predict(rf,iris[,-5])

confusionMatrix(pred,iris[,5])

#####################################################################################################################################

## 75% of the sample size
smp_size <- floor(0.75 * nrow(iris))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(iris)), size = smp_size)

train <- iris[train_ind, ]
test <- iris[-train_ind, ]

rf2 <- randomForest(
  Species ~ . ,
  data=train)

pred2 = predict(rf2,newdata = test[,-5])

confusionMatrix(pred2,test[,5])

#####################################################################################################################################

plot(rf)
plot(rf2)

```

```{r Classification model}
#Support Vector Machine

##easiest code for sVM

library(e1071)
library(caret)

head(iris)

############################################################################################################
#concepts of SVM, Support Vector Matrix

m2 <- svm(Species~Petal.Width+Petal.Length, data = iris,kernel = 'linear')
plot(m2, iris, Petal.Width ~ Petal.Length,
     slice = list(Sepal.Width = 3, Sepal.Length = 4))

m3 <- svm(Species~Sepal.Width+Sepal.Length, data = iris,kernel = 'linear')
plot(m3, iris, Sepal.Width ~ Sepal.Length,
     slice = list(Sepal.Width = 3, Sepal.Length = 4))

m4 <- svm(Species~Sepal.Width+Sepal.Length, data = iris,kernel = 'radial')
plot(m4, iris, Sepal.Width ~ Sepal.Length,
     slice = list(Sepal.Width = 3, Sepal.Length = 4))

m5 <- svm(Species~Sepal.Width+Sepal.Length, data = iris,kernel = 'polynomial')
plot(m5, iris, Sepal.Width ~ Sepal.Length,
     slice = list(Sepal.Width = 3, Sepal.Length = 4))

############################################################################################################
############################################################################################################

x <- iris[, 1:4]
y <- iris[, 5]

head(x)
head(y)

##Create SVM MODEL
svm_model <- svm(x, y, kernel = "polynomial", scale = TRUE)

summary(svm_model)

pred <- predict(svm_model, x)

confusionMatrix(pred, y)

Comparison <- cbind(iris, as.character(pred))


############################################################################################################
############################################################################################################
############################################################################################################


#gamma decides how many points to consider when drawing the decision boundaries

#Gamma and cost 
  #Higher value resulted in over fitting
  #Low value resulted in overly rigid line

#My guess is that gamma controls how sensitive the boundary 
#would be to outliers, while C has to do directly with the geometry of the boundary. 
#For example, you could have an overfitted boundary that is affected only by the border 
#(high gamma, high C) or a completely straight line affected equally 
#by even the farthest points (low gamma, low C)

tuned.svm <- tune.svm(
  x,y,
  kernel = 'polynomial',
  gamma = seq(1 / 2 ^ nrow(iris), 1, .01),
  cost = 2 ^ seq(-6, 4, 2)
)
tuned.svm

tuned.svm.model <-
  svm(
    x,
    y,
    kernel = 'polynomial',
    gamma = tuned.svm$best.parameters$gamma,
    cost = tuned.svm$best.parameters$cost
  )

pred_2 <- predict(tuned.svm.model, x)

confusionMatrix(pred_2, y)

############################################################################################################
############################################################################################################
#actual way to do

svm_model_linear <- svm(x, y)
pred_3 <- predict(svm_model_linear, x,kernel = 'linear')
confusionMatrix(pred_3, y)

```


# Clustering model
1. Hierarchical 
2. Principle Component Analysis
3. T-SNE
4. UMAP
5. K Means


```{r Clustering model}
#Hierarchical Clustering

library(pheatmap)
data(iris)

iris.scaled <- scale(iris[,1:4])

iris.heatmap <- pheatmap(iris.scaled,
                         kmeans_k = NA, 
                         breaks = NA, 
                         border_color = "grey60",
                         cellwidth = NA, 
                         cellheight = NA, 
                         scale = "none", 
                         cluster_rows = TRUE,
                         cluster_cols = TRUE, 
                         clustering_distance_rows = "euclidean",
                         clustering_distance_cols = "euclidean", 
                         clustering_method = "complete",
                         cutree_rows = 3)

```

```{r Clustering model}
#Principle Component Analysis
library(factoextra)
library(FactoMineR)

data(iris)

iris.pca <- PCA(iris[,1:4])

fviz_pca_var(iris.pca)
fviz_pca_ind(iris.pca,repel = TRUE,col.ind = iris$Species)


```

```{r Clustering model}
# T-SNE 
library(Rtsne)
library(ggplot2)

data(iris)

iris.unique <- iris[!duplicated(iris),]
label <- iris.unique[,5]

iris.unique <- as.matrix(iris.unique[,1:4])

iris.rtsne <- Rtsne(iris.unique, dims = 2, initial_dims = 50, perplexity = 30)

plotdata <- as.data.frame(iris.rtsne$Y)

ggplot(plotdata, aes(x=V1,y=V2)) + geom_point(aes(color=label))

```

```{r Clustering model}
#UMAP 

library(umap)
library(ggplot2)

data(iris)

iris.umap <- umap(iris[,1:4])

plotdata <- data.frame(iris.umap$layout)
  
ggplot(plotdata, aes(x=X1,y=X2)) + geom_point(aes(color=iris$Species))
                 

```

```{r Clustering model}
#K Means
#https://uc-r.github.io/kmeans_clustering

library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization

df <- USArrests
df <- na.omit(df)
df <- scale(df)

distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

k2 <- kmeans(df, centers = 2, nstart = 25)
fviz_cluster(k2, data = df)

df %>%
  as_tibble() %>%
  mutate(cluster = k2$cluster,
         state = row.names(USArrests)) %>%
  ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) +
  geom_text()


k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)

set.seed(123)

fviz_nbclust(df, kmeans, method = "wss")

# Compute k-means clustering with k = 4
set.seed(123)
final <- kmeans(df, 4, nstart = 25)
print(final)
fviz_cluster(final, data = df)

```

# Neural Network
1. MLP (Multilayer Perceptron)
2. Convolutional 
3. Recurrent
4. Auto-encoder (Classification and Dimensional Reduction)
5. Etc (RBM, Transformer, and many more)


```{r Neural Network}
#MLP (Multilayer Perceptron)

library(neuralnet)
library(caret)

head(iris)

NN = neuralnet(Species ~ ., iris, hidden = c(10,10,5) )

# plot neural network
plot(NN)

pred = neuralnet::compute(NN, iris[,c(1,2,3,4)])

pred.2 <- data.frame()
for(i in 1:150){
  pred.2 <- rbind(pred.2 ,which.max(pred$net.result[i,]))
}

pred.2$X1L <- gsub(1,"setosa",pred.2$X1L)
pred.2$X1L <- gsub(2,"versicolor",pred.2$X1L)
pred.2$X1L <- gsub(3,"virginica",pred.2$X1L)

prediction <- as.factor(pred.2$X1L)
reference  <- iris[,5]

confusionMatrix(prediction, reference)


```


```{r Neural Network}
#Convolutional 
library(tensorflow)
library(keras)
library(stringr)
library(readr)
library(purrr)
library(caret)

library(e1071)

cifar <- dataset_cifar10()

train_data <- scale(cifar$train$x)
dim(train_data) <- c(50000,32,32,3)

test_data <- scale(cifar$test$x)
dim(test_data) <- c(10000,32,32,3)

train_label <- as.numeric(cifar$train$y)
dim(train_label) <- c(50000)

test_label <- as.numeric(cifar$test$y)
dim(test_label) <- c(10000)

class_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',
                 'dog', 'frog', 'horse', 'ship', 'truck')

index <- 1:30

par(mfcol = c(5,6), mar = rep(1, 4), oma = rep(0.2, 4))

cifar$train$x[index,,,] %>% 
  purrr::array_tree(1) %>%
  purrr::set_names(class_names[cifar$train$y[index] + 1]) %>% 
  purrr::map(as.raster, max = 255) %>%
  purrr::iwalk(~{plot(.x); title(.y)})

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", input_shape = c(32,32,3)) %>% 
  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) 

summary(model)

model %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")

summary(model)

model %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)

history <- model %>% 
  fit(
    x = train_data, y = train_label,
    epochs = 2,
    validation_split=0.2,
    use_multiprocessing=TRUE
  )

plot(history)

############################################################################
Prediction_train_data <- predict_classes(model, train_data)
confusionMatrix(table(Prediction_train_data,train_label))


############################################################################
Prediction_test_data <- predict_classes(model, test_data)
confusionMatrix(table(Prediction_test_data,test_label))


```



```{r Neural Network}
#Recurrent
#https://github.com/brandonyph/Introduction-to-RNN-in-R

library(tibble)
library(readr)
library(ggplot2)
library(keras)
library(tensorflow)

raw_data <- read_csv("https://raw.githubusercontent.com/gilbutITbook/006975/master/datasets/jena_climate/jena_climate_2009_2016.csv")

data <- data.matrix(raw_data[, -1])

mean <- apply(data, 2, mean)
std <- apply(data, 2, sd)
data <- scale(data, center = mean, scale = std)

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

max <- apply(data,2,max)
min <- apply(data,2,min)

data <- apply(data, 2, normalize)

plot(data[1:30000,2 ])

generator <- function(data,
                      lookback,
                      delay,
                      min_index,
                      max_index,
                      shuffle = FALSE,
                      batch_size = 128,
                      step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <-
        sample(c((min_index + lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i + batch_size - 1, max_index))
      i <<- i + length(rows)
    }
    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1,
                     length.out = dim(samples)[[2]])
      samples[j, , ] <- data[indices, ]
      targets[[j]] <- data[rows[[j]] + delay, 2]
    }
    list(samples, targets)
  }
}

lookback <- 1440
step <- 6
delay <- 44
batch_size <- 64

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 30000,
  shuffle = FALSE,
  step = step,
  batch_size = batch_size)

lookback <- 240
step <- 1
delay <- 44
batch_size <- 64

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 30000,
  shuffle = FALSE,
  step = step,
  batch_size = batch_size)

train_gen_data <- train_gen()

model <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1)

summary(model)

model %>% compile(optimizer = optimizer_rmsprop(),
                  loss = "mae")

history <- model %>% fit(
  train_gen_data[[1]],train_gen_data[[2]],
  batch_size = 32,
  epochs = 20,
  use_multiprocessing = T
)

 batch_size_plot <- 17600
  lookback_plot <- lookback
  step_plot <- 1 
  
  pred_gen <- generator(
    data,
    lookback = lookback_plot,
    delay = 0,
    min_index = 30000,
    max_index = 50000,
    shuffle = FALSE,
    step = step_plot,
    batch_size = batch_size_plot
  )
  
  pred_gen_data <- pred_gen()
  
  V1 = seq(1, length(pred_gen_data[[2]]))
  
  plot_data <-
    as.data.frame(cbind(V1, pred_gen_data[[2]]))
  
  inputdata <- pred_gen_data[[1]]
  dim(inputdata) <- c(batch_size_plot, lookback, 14)
  
  pred_out <- model %>%
    predict(inputdata) 
  
  plot_data <-
    cbind(plot_data, pred_out)
  
  p <-
    ggplot(plot_data, aes(x = V1, y = V2)) + geom_point(colour = "blue", size = 0.1,alpha=0.4)
  p <-
    p + geom_point(aes(x = V1, y = pred_out), colour = "red", size = 0.1 ,alpha=0.4)
  
  p

```


```{r Neural Network}
#Auto-encoder
#https://www.r-bloggers.com/2018/07/pca-vs-autoencoders-for-dimensionality-reduction/

suppressPackageStartupMessages(library(DAAG))
head(ais)

# standardise
minmax <- function(x) (x - min(x))/(max(x) - min(x))
x_train <- apply(ais[,1:11], 2, minmax)

# autoencoder in keras
suppressPackageStartupMessages(library(keras))
# set training data
x_train <- as.matrix(x_train)

# autoencoder in keras
library(keras)
# set training data
###
model3 <- keras_model_sequential()
model3 %>%
  layer_dense(units = 6, activation = "tanh", input_shape = ncol(x_train)) %>%
  layer_dense(units = 3, activation = "tanh", name = "bottleneck") %>%
  layer_dense(units = 6, activation = "tanh") %>%
  layer_dense(units = ncol(x_train))
# summary of model
summary(model3)

# compile model
model3 %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam"
)
# fit model
model3 %>% fit(
  x = x_train, 
  y = x_train, 
  verbose = 0,
  epochs = 100,
  batch_size = 2
)
# evaluate the performance of the model
mse.ae2 <- evaluate(model3, x_train, x_train)
mse.ae2

# exgtract the bottleneck layer
intermediate_layer_model <- keras_model(inputs = model3$input, outputs = get_layer(model3, "bottleneck")$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
# plot the reduced dat set
aedf3 <- data.frame(node1 = intermediate_output[,1], node2 = intermediate_output[,2], node3 = intermediate_output[,3])


library(plotly)
ae_plotly <- plot_ly(aedf3, x = ~node1, y = ~node2, z = ~node3, color = ~ais$sex) %>% add_markers()
ae_plotly 

```














